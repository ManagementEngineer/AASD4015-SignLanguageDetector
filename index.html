<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA);ol{margin:0;padding:0}table td,table th{padding:0}.c2{background-color:#ffffff;color:#212121;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Roboto";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c5{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c11{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c12{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c17{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c9{background-color:#ffffff;font-size:10pt;font-family:"Roboto";color:#212121;font-weight:400}.c14{background-color:#ffffff;font-weight:400;font-size:10pt;font-family:"Roboto"}.c13{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c15{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{color:inherit;text-decoration:inherit}.c16{font-style:italic}.c8{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c15 doc-content"><p class="c11 title" id="h.ew1q6oojqo6"><span class="c10">Final Report of Project 1 and 2</span></p><h3 class="c7" id="h.76d0o427sth5"><span class="c5">Advanced Applied Mathematical Concepts for Deep Learning II</span></h3><h3 class="c7" id="h.63t5bxlyphf4"><span class="c5">Title:</span></h3><p class="c17 subtitle" id="h.ibj6nydy5lh"><span class="c12">Digits of the American Hand Sign Language Image Classifier</span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.edrqdc81nh1e"><span class="c5">Group members:</span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">Ramuel Batuigas</span></p><p class="c4"><span class="c0">Shreeya Shah</span></p><p class="c4"><span class="c0">Aanandita Chavan</span></p><p class="c4"><span class="c0">Wing Han Yiu</span></p><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.g3olz9waq4mp"><span class="c5">Abstract: </span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">In this project of object detection by using various types of models, it is associated with the hand signs from American sign languages. It is typically addressing challenges that the underprivileged groups may have by technology. It helps build the communication bridge for people to access and interact with the individuals and get to know more about their needs immediately, apart from the inclusion in the society but also some healthcare emergencies. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">In this convolutional neural network task, images of the hand signs would be used to train and test the model, with the preprocessed and annotated data, a Streamlit App on cloud will be developed to demonstrate and help identifying the hand signs images that the users upload. The model will be hyper-parameter tuned for accuracy and flexibility due to several models being applied and compared. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">The final report in the following is going to demonstrate the procedures of the development of the product.</span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.fjn78j37n8if"><span class="c5">Introduction: </span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">In this project, images of the hand signs from the American sign languages will be applied and analyzed with the model developed then demonstrated with the Streamlit app.</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">It is observable that there are still some communication barriers of the sign language users in the society, such as the lack of awareness and knowledge from the people who do not know sign languages and also the limited access of the information for translation. It leads to inconvenience and the isolation between the people and sign language users. [1]</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span>With the assistance of technology, it enhances the sign language user-friendly environment for communication and the promotion and education of sign languages to the public. It helps connect the people who have hearing or speaking problems to the society and bridges the people who are keen on helping them but with lack of resources and tools. With the model developed, the sign language users may not rely on the </span><span>interpreters</span><span class="c0">&nbsp;but can use it at any time. The cost of the communication reduces by time and human resources.</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">In this project, several models have been used and applied to develop the app. Arabic numbers of the sign languages will be identified. Although the scale of the model is limited due to some obstacles and situations, for example only numbers analyzed within a time limit, yet, it has potential to be developed with further and widespread applications in the future.</span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.32knv0346tv1"><span class="c5">Related Work: </span></h3><p class="c3"><span class="c1"></span></p><p class="c4"><span class="c1">Discuss work that relates to your project. How is your approach similar or different from others? (for example the starter code you might have used; Kaggle notebooks, etc.)</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">The project is referred by the </span></p><h3 class="c7" id="h.n053ognfxv3p"><span class="c5">Data: </span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span>Link to the dataset used: </span><span class="c13"><a class="c6" href="https://www.google.com/url?q=https://www.kaggle.com/datasets/victoranthony/asl-digits-0-9&amp;sa=D&amp;source=editors&amp;ust=1691295115003798&amp;usg=AOvVaw2eJYIu_X61VXSo3AxR3raT">https://www.kaggle.com/datasets/victoranthony/asl-digits-0-9</a></span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">The model developed is based on the American sign languages dataset retrieved from Kaggle. In the dataset, it includes the number 0 to 9, with 10 classes in total. It contains a total 570 and 130 images with 400X400 pixels in resolution for training and testing sets respectively. Each class has 57 and 13 &nbsp;images of certain digits in the training and testing sets with the correct labels of the signs represented.</span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.6ga3r9iozqn0"><span class="c5">Methods: </span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">Several models have been applied and used to develop, for example, the VGG-16, the InceptionNet and also the YOLOv8. They are all the image recognition CNN models with various differences. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c8">VGG-16</span><span class="c0">&nbsp;is a convolutional neural network which has 16 layers depth. It has the pretrained version of the model with more than a million images from the ImageNet database. [2] It is able to classify images in 1000 object categories. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c8">InceptionNet</span><span class="c0">&nbsp;is developed by Google to improve the performance of the previous CNNs. It basically applies the inception modules with the combination of 1x1, 3x3 and 5x5 convolutions on the input data and the auxiliary classifiers. It is able to do image classification, object detection and image segmentation.[3]</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c8">YOLOv8</span><span class="c0">&nbsp;is a model that enables object detection and instance segmentation, developed by Ultralytics. The You Only Look Once series of models are famous on doing computer vision task because of its high rate of accuracy and anchor free detection which reduces the number of box predictions which speeds up the post processing steps of candidate detection after inference. [4]</span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.s7orc2lsb87x"><span class="c5">Experiments: </span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c1">Discuss and describe the experiments that you performed to demonstrate that your approach solves the problem. The exact experiments will vary depending on the project, but you might compare with previously published methods, perform an ablation study to determine the impact of various components of your system, experiment with different hyperparameters or architectural choices, use visualization techniques to gain insight into how your model works, discuss common failure modes of your model, etc. You should include graphs, tables, or other figures to illustrate your experimental results. This could include a Streamlit App. In such a case, the app will stand for up to 30% of the grade, on expense of the experiment section.</span></p><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">Results:</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">Hyperparameter tuning:</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">Accuracy:</span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">Streamlit app demonstration:</span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.2d3h50od2qlt"><span class="c5">Conclusion:</span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">In this project, a Streamlit app for identifying the number of hand sign language is developed. With the images uploaded, several models are applied to digit detection, including VGG-16, InceptionNet and YOLOv8 with different results of performances. </span></p><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c0">The topic has potential to be further developed in the future based on the model it has developed here, such as the alphabets and translation of images to audio or written texts. Hence, it brings further convenience, inclusion and coherence with further accommodations in the future society.</span></p><p class="c3"><span class="c0"></span></p><p class="c3"><span class="c0"></span></p><h3 class="c7" id="h.e131l3avi4kh"><span class="c5">Reference:</span></h3><p class="c3"><span class="c0"></span></p><p class="c4"><span class="c2">[1] Challenges That Still Exist for the Deaf Community. https://www.verywellhealth.com/what-challenges-still-exist-for-the-deaf-community-4153447 </span></p><p class="c3"><span class="c2"></span></p><p class="c4"><span class="c9">[2] </span><span class="c9 c16">ImageNet</span><span class="c9">. </span><span class="c14 c13"><a class="c6" href="https://www.google.com/url?q=http://www.image-net.org&amp;sa=D&amp;source=editors&amp;ust=1691295115006537&amp;usg=AOvVaw3wabgajEjDQEpvbyRzQiIW">http://www.image-net.org</a></span></p><p class="c3"><span class="c2"></span></p><p class="c4"><span class="c9">[3] Introduction to InceptionNet. </span><span class="c14 c13"><a class="c6" href="https://www.google.com/url?q=https://www.scaler.com/topics/inception-network/&amp;sa=D&amp;source=editors&amp;ust=1691295115006960&amp;usg=AOvVaw2syXUqiKzjC24vW9QvJPoB">https://www.scaler.com/topics/inception-network/</a></span></p><p class="c3"><span class="c2"></span></p><p class="c4"><span class="c9">[4] What is YOLOv8? The Ultimate Guide. </span><span class="c13 c14"><a class="c6" href="https://www.google.com/url?q=https://blog.roboflow.com/whats-new-in-yolov8/&amp;sa=D&amp;source=editors&amp;ust=1691295115007381&amp;usg=AOvVaw2yYiT98586KNpCiS97xgQC">https://blog.roboflow.com/whats-new-in-yolov8/</a></span></p><p class="c3"><span class="c2"></span></p></body></html>